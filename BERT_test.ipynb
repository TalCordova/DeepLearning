{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0t1VgfkCowKA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        line = f.readline()\n",
        "        if not line:\n",
        "            return None\n",
        "        sentence = []\n",
        "        while line and (line != \"\\n\"):\n",
        "            line = line.strip()\n",
        "            sentence.append(line)\n",
        "            line = f.readline()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "xzWuqYIwpkZJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_data(train_data, train_labels, test_data, test_labels):\n",
        "\n",
        "    # ~ Encode labels\n",
        "    labels = np.unique(np.array(train_labels)).tolist() + ['UNK']\n",
        "    labels_mapping = dict(zip(labels, np.arange(len(labels))))\n",
        "\n",
        "    # Normalize\n",
        "    normalized_train_data = normalize_data(train_data)\n",
        "\n",
        "    # Get labels\n",
        "    train_labels = [labels_mapping[label] for label in train_labels]\n",
        "\n",
        "    # Get train data\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    train_data = [tokenizer(text,\n",
        "                            padding='max_length',\n",
        "                            max_length=512,\n",
        "                            truncation=True,\n",
        "                            return_tensors='pt')\n",
        "                  for text in normalized_train_data]\n",
        "\n",
        "\n",
        "    # TEST Related\n",
        "    normalized_test_data = normalize_data(test_data)\n",
        "    test_labels = ['UNK' if label not in labels else label for label in test_labels]\n",
        "    test_labels = [labels_mapping[label] for label in test_labels]\n",
        "    test_data = [tokenizer(text,\n",
        "                           padding='max_length',\n",
        "                           max_length=512,\n",
        "                           truncation=True,\n",
        "                           return_tensors='pt')\n",
        "                 for text in normalized_test_data]\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels\n"
      ],
      "metadata": {
        "id": "AJMHoFYLpmh6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(list_of_texts):\n",
        "    list_of_normalized_texts = []\n",
        "\n",
        "    # Normalize each of the tweets\n",
        "    for text in list_of_texts:\n",
        "        # Lower the text\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Remove punctuation\n",
        "        text_no_punctuation = re.sub(r'[^\\w\\s]', '', text_lower)\n",
        "\n",
        "        # Split to tokens\n",
        "        splitted_text = text_no_punctuation.split(' ')\n",
        "\n",
        "        # Replace numbers with <number> token\n",
        "        text_number_token = ['<number>' if bool(re.search(r'\\d', x)) else x for x in splitted_text]\n",
        "\n",
        "        # Remove stop words\n",
        "        text_no_stop_words = [x for x in text_number_token if x not in nltk.corpus.stopwords.words('english')]\n",
        "\n",
        "        # Join to string\n",
        "        joined_text = ' '.join(text_no_stop_words)\n",
        "\n",
        "        # Append to the list\n",
        "        list_of_normalized_texts.append(joined_text)\n",
        "\n",
        "    return list_of_normalized_texts\n"
      ],
      "metadata": {
        "id": "a4GRcwSIpomD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, train_labels, test_data, test_labels, learning_rate, epochs):\n",
        "\n",
        "    train = Dataset(train_data, train_labels)\n",
        "    test = Dataset(test_data, test_labels)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=16, shuffle=True)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "\n",
        "        for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_input['attention_mask'].to(device)\n",
        "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "\n",
        "            batch_loss = criterion(output, train_label)\n",
        "            total_loss_train += batch_loss.item()\n",
        "\n",
        "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "            total_acc_train += acc\n",
        "\n",
        "            model.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for test_input, test_label in test_dataloader:\n",
        "\n",
        "                test_label = test_label.to(device)\n",
        "                mask = test_input['attention_mask'].to(device)\n",
        "                input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(output, test_label)\n",
        "                total_loss_val += batch_loss.item()\n",
        "\n",
        "                acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "                total_acc_val += acc\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(test_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(test_data): .3f}')\n"
      ],
      "metadata": {
        "id": "lfuCUfNeptOk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data, labels):\n",
        "\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "        self.texts = data\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "wElV_xKMputf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 22)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "uAHrNBIupxPn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and labels\n",
        "train_data = read_data(r'./atis_data/train/seq.in')\n",
        "train_labels = read_data(r'./atis_data/train/label')\n",
        "test_data = read_data(r'./atis_data/test/seq.in')\n",
        "test_labels = read_data(r'./atis_data/test/label')\n",
        "\n",
        "train_data, train_labels, test_data, test_labels = preprocess_text_data(train_data, train_labels, test_data, test_labels)\n",
        "\n",
        "EPOCHS = 10\n",
        "model = BertClassifier()\n",
        "LR = 1e-4\n",
        "\n",
        "train(model, train_data, train_labels, test_data, test_labels, LR, EPOCHS)\n"
      ],
      "metadata": {
        "id": "QLH0tYmjpy5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd83196-4c69-4a78-9808-e228f4421390"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 280/280 [06:15<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.039                 | Train Accuracy:  0.864                 | Val Loss:  0.038                 | Val Accuracy:  0.889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 280/280 [06:34<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.017                 | Train Accuracy:  0.943                 | Val Loss:  0.031                 | Val Accuracy:  0.887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 61/280 [01:25<05:08,  1.41s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A3-iPqF0sFLp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}