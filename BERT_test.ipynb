{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BERT_test.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0t1VgfkCowKA"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            return None\n",
    "        sentence = []\n",
    "        while line and (line != \"\\n\"):\n",
    "            line = line.strip()\n",
    "            sentence.append(line)\n",
    "            line = f.readline()\n",
    "    return sentence"
   ],
   "metadata": {
    "id": "xzWuqYIwpkZJ"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_text_data(train_data, train_labels, test_data, test_labels):\n",
    "\n",
    "    # ~ Encode labels\n",
    "    labels = np.unique(np.array(train_labels)).tolist() + ['UNK']\n",
    "    labels_mapping = dict(zip(labels, np.arange(len(labels))))\n",
    "\n",
    "    # Normalize\n",
    "    normalized_train_data = normalize_data(train_data)\n",
    "\n",
    "    # Get labels\n",
    "    train_labels = [labels_mapping[label] for label in train_labels]\n",
    "\n",
    "    # Get train data\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    train_data = [tokenizer(text,\n",
    "                            padding='max_length',\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            return_tensors='pt')\n",
    "                  for text in normalized_train_data]\n",
    "\n",
    "\n",
    "    # TEST Related\n",
    "    normalized_test_data = normalize_data(test_data)\n",
    "    test_labels = ['UNK' if label not in labels else label for label in test_labels]\n",
    "    test_labels = [labels_mapping[label] for label in test_labels]\n",
    "    test_data = [tokenizer(text,\n",
    "                           padding='max_length',\n",
    "                           max_length=512,\n",
    "                           truncation=True,\n",
    "                           return_tensors='pt')\n",
    "                 for text in normalized_test_data]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n"
   ],
   "metadata": {
    "id": "AJMHoFYLpmh6"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def normalize_data(list_of_texts):\n",
    "    list_of_normalized_texts = []\n",
    "\n",
    "    # Normalize each of the tweets\n",
    "    for text in list_of_texts:\n",
    "        # Lower the text\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text_no_punctuation = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "\n",
    "        # Split to tokens\n",
    "        splitted_text = text_no_punctuation.split(' ')\n",
    "\n",
    "        # Replace numbers with <number> token\n",
    "        text_number_token = ['<number>' if bool(re.search(r'\\d', x)) else x for x in splitted_text]\n",
    "\n",
    "        # Remove stop words\n",
    "        text_no_stop_words = [x for x in text_number_token if x not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "        # Join to string\n",
    "        joined_text = ' '.join(text_no_stop_words)\n",
    "\n",
    "        # Append to the list\n",
    "        list_of_normalized_texts.append(joined_text)\n",
    "\n",
    "    return list_of_normalized_texts\n"
   ],
   "metadata": {
    "id": "a4GRcwSIpomD"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, train_data, train_labels, test_data, test_labels, learning_rate, epochs):\n",
    "\n",
    "    train = Dataset(train_data, train_labels)\n",
    "    test = Dataset(test_data, test_labels)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=16, shuffle=True)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_input, test_label in test_dataloader:\n",
    "\n",
    "                test_label = test_label.to(device)\n",
    "                mask = test_input['attention_mask'].to(device)\n",
    "                input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, test_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(test_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(test_data): .3f}')\n"
   ],
   "metadata": {
    "id": "lfuCUfNeptOk"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.texts = data\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ],
   "metadata": {
    "id": "wElV_xKMputf"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 22)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ],
   "metadata": {
    "id": "uAHrNBIupxPn"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load data and labels\n",
    "train_data = read_data('atis/train/seq.in')\n",
    "train_labels = read_data('atis/train/label')\n",
    "test_data = read_data('atis/test/seq.in')\n",
    "test_labels = read_data('atis/test/label')\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = preprocess_text_data(train_data, train_labels, test_data, test_labels)\n",
    "\n",
    "EPOCHS = 10\n",
    "model = BertClassifier()\n",
    "LR = 1e-4\n",
    "\n",
    "train(model, train_data, train_labels, test_data, test_labels, LR, EPOCHS)\n"
   ],
   "metadata": {
    "id": "QLH0tYmjpy5U",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9bd83196-4c69-4a78-9808-e228f4421390"
   },
   "execution_count": 24,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jvjos/nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001B[0m in \u001B[0;36m__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     83\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 84\u001B[1;33m                     \u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{self.subdir}/{zip_name}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     85\u001B[0m                 \u001B[1;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    582\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 583\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    584\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords.zip/stopwords/\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jvjos/nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20112/1626854575.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mtest_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mread_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'atis/test/label'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreprocess_text_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_labels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mEPOCHS\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20112/4238703558.py\u001B[0m in \u001B[0;36mpreprocess_text_data\u001B[1;34m(train_data, train_labels, test_data, test_labels)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;31m# Normalize\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mnormalized_train_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnormalize_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[1;31m# Get labels\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20112/2080941118.py\u001B[0m in \u001B[0;36mnormalize_data\u001B[1;34m(list_of_texts)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m         \u001B[1;31m# Remove stop words\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[0mtext_no_stop_words\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtext_number_token\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstopwords\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'english'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;31m# Join to string\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20112/2080941118.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m         \u001B[1;31m# Remove stop words\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[0mtext_no_stop_words\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mx\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtext_number_token\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstopwords\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'english'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;31m# Join to string\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    119\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 121\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    122\u001B[0m         \u001B[1;31m# This looks circular, but its not, since __load() changes our\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    123\u001B[0m         \u001B[1;31m# __class__ to something new:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001B[0m in \u001B[0;36m__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     84\u001B[0m                     \u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{self.subdir}/{zip_name}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m                 \u001B[1;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 86\u001B[1;33m                     \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     87\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     88\u001B[0m         \u001B[1;31m# Load the corpus.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001B[0m in \u001B[0;36m__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 81\u001B[1;33m                 \u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"{self.subdir}/{self.__name}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     82\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    581\u001B[0m     \u001B[0msep\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"*\"\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m70\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    582\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 583\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    584\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    585\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mstopwords\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mcorpora/stopwords\u001B[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jvjos/nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jvjos\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "A3-iPqF0sFLp"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}