{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2 - Question 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load CIFAR10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# todo: change the root param\n",
    "dataset_train = CIFAR10(root='C:/Users/jvjos/Desktop/Tal/לימודים/למידה עמוקה/ex_2/', download=True, transform=ToTensor())\n",
    "# todo: change the root param\n",
    "dataset_test = CIFAR10(root='C:/Users/jvjos/Desktop/Tal/לימודים/למידה עמוקה/ex_2/', download=False, train=False, transform=ToTensor())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ANN and CNN classes from PyTorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, img_size=32, hidden_size=128, output_dimension=10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Linear1 = nn.Linear(img_size**2*3, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.Linear2 = nn.Linear(hidden_size, int(hidden_size/2))\n",
    "        self.bn2 = nn.BatchNorm1d(int(hidden_size/2))\n",
    "        self.output = nn.Linear(int(hidden_size/2), output_dimension)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, input):\n",
    "        flatten = self.flatten(input)\n",
    "        Linear1 = self.Linear1(flatten)\n",
    "        bn1 = self.bn1(Linear1)\n",
    "        actvivition1 = nn.ReLU()(bn1)\n",
    "        dropout1 = self.dropout(actvivition1)\n",
    "        Linear2 = self.Linear2(dropout1)\n",
    "        bn2 = self.bn2(Linear2)\n",
    "        actvivition2 = nn.ReLU()(bn2)\n",
    "        dropout2 = self.dropout(actvivition2)\n",
    "        output = self.output(dropout2)\n",
    "        return nn.Softmax(dim=1)(output)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, img_size=32, hidden_size=128, output_dimension=10):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(7, 7))\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(7, 7))\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(7, 7))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.Linear1 = nn.Linear(12544, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.Linear2 = nn.Linear(hidden_size, int(hidden_size/2))\n",
    "        self.bn2 = nn.BatchNorm1d(int(hidden_size/2))\n",
    "        self.output = nn.Linear(int(hidden_size/2), output_dimension)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, input):\n",
    "        torch.swapdims(input, 1, -1)\n",
    "        conv2d_1 = self.conv2d_1(input)\n",
    "        relu_1 = nn.ReLU()(conv2d_1)\n",
    "        conv2d_2 = self.conv2d_2(relu_1)\n",
    "        relu_2 = nn.ReLU()(conv2d_2)\n",
    "        conv2d_3 = self.conv2d_3(relu_2)\n",
    "        relu_3 = nn.ReLU()(conv2d_3)\n",
    "        flatten = self.flatten(relu_3)\n",
    "        Linear1 = self.Linear1(flatten)\n",
    "        bn1 = self.bn1(Linear1)\n",
    "        actvivition1 = nn.ReLU()(bn1)\n",
    "        dropout1 = self.dropout(actvivition1)\n",
    "        Linear2 = self.Linear2(dropout1)\n",
    "        bn2 = self.bn2(Linear2)\n",
    "        actvivition2 = nn.ReLU()(bn2)\n",
    "        dropout2 = self.dropout(actvivition2)\n",
    "        output = self.output(dropout2)\n",
    "        return nn.Softmax(dim=1)(output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    for X, y in tqdm(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, f'{100*correct:.2f}%'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification Using ANN and CNN\n",
    "Resutlts are in attached csv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 35.5%, Avg loss: 2.098243 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.076081 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.056944 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 2.057711 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 2.060729 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 2.055142 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 2.046571 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 2.037368 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 2.045954 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 2.036740 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.131555 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.104474 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.3%, Avg loss: 2.081608 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.7%, Avg loss: 2.078084 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 35.8%, Avg loss: 2.095944 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.067788 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 39.2%, Avg loss: 2.059931 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 2.066936 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 41.2%, Avg loss: 2.043281 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 39.7%, Avg loss: 2.059904 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:09<00:00, 85.02it/s]\n",
      "100%|██████████| 782/782 [00:08<00:00, 87.08it/s]\n",
      "100%|██████████| 782/782 [00:09<00:00, 86.74it/s]\n",
      "100%|██████████| 782/782 [00:09<00:00, 86.68it/s]\n",
      "100%|██████████| 782/782 [00:08<00:00, 87.75it/s]\n",
      "100%|██████████| 782/782 [00:08<00:00, 90.44it/s]\n",
      "100%|██████████| 782/782 [00:08<00:00, 87.15it/s]\n",
      "100%|██████████| 782/782 [00:08<00:00, 88.26it/s]\n",
      "100%|██████████| 782/782 [00:08<00:00, 88.84it/s]\n",
      "100%|██████████| 782/782 [00:08<00:00, 87.35it/s]\n",
      "100%|██████████| 782/782 [00:53<00:00, 14.63it/s]\n",
      "100%|██████████| 782/782 [00:55<00:00, 14.05it/s]\n",
      "100%|██████████| 782/782 [00:55<00:00, 14.04it/s]\n",
      "100%|██████████| 782/782 [00:56<00:00, 13.96it/s]\n",
      "100%|██████████| 782/782 [00:56<00:00, 13.92it/s]\n",
      "100%|██████████| 782/782 [00:56<00:00, 13.83it/s]\n",
      "100%|██████████| 782/782 [00:56<00:00, 13.72it/s]\n",
      "100%|██████████| 782/782 [00:56<00:00, 13.75it/s]\n",
      "100%|██████████| 782/782 [00:57<00:00, 13.71it/s]\n",
      "100%|██████████| 782/782 [00:57<00:00, 13.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# data loaders\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=64, shuffle=True)\n",
    "\n",
    "# train ANN model\n",
    "model = ANN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# init dataframe to save results\n",
    "res = {'model': [], 'test_loss': [], 'test_acc': []}\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss, test_acc = test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "# save ANN results\n",
    "res['model'].append('ANN')\n",
    "res['test_loss'].append(test_loss)\n",
    "res['test_acc'].append(test_acc)\n",
    "\n",
    "# train CNN model\n",
    "model = CNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# init dataframe to save results\n",
    "res = {'model': [], 'test_loss': [], 'test_acc': []}\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss, test_acc = test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "# save ANN results\n",
    "res['model'].append('CNN')\n",
    "res['test_loss'].append(test_loss)\n",
    "res['test_acc'].append(test_acc)\n",
    "\n",
    "# save dataframe to csv\n",
    "df = pd.DataFrame(res)\n",
    "df.to_csv('results.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}